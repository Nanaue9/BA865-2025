{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNMKDe3TWra86j3nYsRoCPW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Generative Text Model Trained on a Corpus of Shakespear**\n","\n","\n","\n"],"metadata":{"id":"iqBsw2Dyo1BA"}},{"cell_type":"markdown","source":["Let's download some Shakespear"],"metadata":{"id":"OAdMvgp2cxkL"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"o7R5wj0jhcY1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744930267259,"user_tz":240,"elapsed":6037,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"f5c334f0-e877-418d-88fd-d39e16beb215"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"]}],"source":["import keras\n","\n","filename = keras.utils.get_file(\n","    origin=\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",")\n","shakespeare = open(filename, \"r\").read()"]},{"cell_type":"markdown","source":["Let's see what's in the corpus..."],"metadata":{"id":"X8BvErp56kdm"}},{"cell_type":"code","source":["# The first 250 characters of text... including line breaks.\n","print(shakespeare[:250])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Haw1re3_6lzp","executionInfo":{"status":"ok","timestamp":1744930273167,"user_tz":240,"elapsed":211,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"8af7e754-1b95-4663-e512-1bcdb277caa8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"]}]},{"cell_type":"markdown","source":["Let's make some training observations..."],"metadata":{"id":"uFvjAtrpoxb-"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Each observation will be 100 characters long.\n","sequence_length = 100\n","\n","# We take the original document and we split it into 100 character sequence chunks.\n","def split_input(input, sequence_length):\n","\n","    # Loop over each 100-character segment of the text\n","    for i in range(0, len(input), sequence_length):\n","\n","        # Return a list of those text chunks.\n","        yield input[i:i + sequence_length]\n","\n","# Features (x's) are mutually exclusive blocks / chunks of 100 characters from the original text.\n","features = list(split_input(shakespeare[:-1], sequence_length))\n","\n","# Labels are also mutually exclusive blocks / chunks of 100 characters, offset from the input sequence by 1 character.\n","# The goal of this training model is to learn weights for our embedding layer and an RNN that we can use in our generative model\n","# For single token predictions.\n","# Our training model will be a throw-away model that generates 100 next-token predictions at a time.\n","labels = list(split_input(shakespeare[1:], sequence_length))\n","\n","# We make a Tensorflow Dataset from this.\n","dataset = tf.data.Dataset.from_tensor_slices((features, labels))"],"metadata":{"id":"_8dsXMrbk445","executionInfo":{"status":"ok","timestamp":1744934298093,"user_tz":240,"elapsed":55,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Here are the first two observations"],"metadata":{"id":"lkU1pZC_8pcy"}},{"cell_type":"code","source":["for i, (features, labels) in enumerate(dataset.as_numpy_iterator()):\n","  print(features[:20])\n","  print(labels[:20])\n","  if i==1:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaOEY6UW8JvX","executionInfo":{"status":"ok","timestamp":1744934302861,"user_tz":240,"elapsed":18,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"1b2c94dd-685c-4da9-980d-e6426ab145ea"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:\\nBefor'\n","b'irst Citizen:\\nBefore'\n","b' are all resolved ra'\n","b'are all resolved rat'\n"]}]},{"cell_type":"markdown","source":["We now use a TextVectorization layer to split out individual characters in each sequence and convert them into numeric values (integer indices). We apply that layer to our Tensorflow Dataset. We will work with a character-level model becuase there are way fewer values to predict in our softmax this way :). We have 26 letters * 2 (upper vs. lower case), plus punctuation, spaces, line breaks and such. Indeed, we have just 67 unique characters in the entire document."],"metadata":{"id":"hmra5UFwVlwl"}},{"cell_type":"code","source":["from keras import layers\n","\n","# Define the layer\n","tokenizer = layers.TextVectorization(\n","    standardize=None,\n","    split=\"character\",\n","    output_sequence_length=sequence_length,\n",")\n","\n","# Apply it to the text we pull from our dataset (dropping the labels since we don't need those when adapting the TextVectorization layer)\n","tokenizer.adapt(dataset.map(lambda text, labels: text))\n","\n","vocabulary_size = tokenizer.vocabulary_size()\n","\n","print(f'We have {vocabulary_size} unique characters in our document.')"],"metadata":{"id":"8sEQSBHNiGSo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744934356487,"user_tz":240,"elapsed":50492,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"2a385182-22cd-4b9c-dcb5-b87fecc4683e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 67 unique characters in our document.\n"]}]},{"cell_type":"markdown","source":["Now we can use the tokenizer to tokenize the text coming out of the Dataset."],"metadata":{"id":"rbGGoRwVakNX"}},{"cell_type":"code","source":["dataset = dataset.map(\n","    lambda features, labels: (tokenizer(features), tokenizer(labels)),\n","    num_parallel_calls=8,\n",")\n","\n","# We have to shuffle the data up front here because we are using a Tensorflow dataset object.\n","# Unlike when all our data is sitting in memory, model.fit(shuffle=True) will not shuffle the order of observations\n","# when we have a Dataset object as input. This is because the fit() function only has access to one batch at a time.\n","# It actually just shuffles the order of batches that it pulls from the Dataset.\n","# In contrast, when data is all in memory, the model.fit() call shuffles observations ahead of time, before creating batches.\n","training_data = dataset.batch(64).cache().shuffle(10_000)\n","\n","print(f'We have created a dataset object that has {dataset.cardinality()} observations.')"],"metadata":{"id":"atP07SgPMReo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744934896237,"user_tz":240,"elapsed":129,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"b537d0df-b707-464d-c617-481ce1d028ef"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["We have created a dataset object that has 11154 observations.\n"]}]},{"cell_type":"markdown","source":["#*The Architecture for our Throw-away Language Model...*"],"metadata":{"id":"4d-yC3vbAzqM"}},{"cell_type":"code","source":["# Input will be 100 integer values per sequence (x implicit batch size)\n","inputs = layers.Input(shape=(sequence_length,), dtype=\"int\", name=\"integer_seq_input\")\n","\n","# We project each integer index into a 256 dimensional embedding space.\n","x = layers.Embedding(input_dim=vocabulary_size, output_dim=256)(inputs)\n","\n","# We pass the sequence of embeddings into an RNN (a GRU in this case).\n","# We have 1,024 GRU units here (so we will obtain 1,024 scalar values at each step)\n","# Again, the hidden state at each point in the sequence can be thought of as a form of embedding for the sequence up to that point.\n","x = layers.GRU(1024, return_sequences=True)(x)\n","\n","# We then have a bit of dropout\n","x = layers.Dropout(0.1)(x)\n","\n","# And a softmax prediction among the 67 unique characters, the shape of this will be 100 characters by 67 softmax values, per input observation.\n","outputs = layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs, outputs)\n","\n","# For all 100 characters that enter the model in a given observation...\n","# we make parallel predictions for the character that should come next using a softmax.\n","# Notice the shape of our output layer is 100, 67-dimensional predictions per observation.\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"wgzGWgtI_9yp","executionInfo":{"status":"ok","timestamp":1744934900460,"user_tz":240,"elapsed":839,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"4e8e8795-71f2-4253-cf84-482dd51ab206"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ integer_seq_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m17,152\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │     \u001b[38;5;34m3,938,304\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m67\u001b[0m)        │        \u001b[38;5;34m68,675\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ integer_seq_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">67</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">68,675</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,024,131\u001b[0m (15.35 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,024,131</span> (15.35 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,024,131\u001b[0m (15.35 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,024,131</span> (15.35 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"sparse_categorical_accuracy\"],\n",")\n","\n","model.fit(training_data, epochs=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U41kVgt8Eise","executionInfo":{"status":"ok","timestamp":1744934945801,"user_tz":240,"elapsed":42422,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"264b0e0b-cc6e-472d-ab93-ffb23ae75644"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 3.0730 - sparse_categorical_accuracy: 0.2397\n","Epoch 2/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.9993 - sparse_categorical_accuracy: 0.4127\n","Epoch 3/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.7402 - sparse_categorical_accuracy: 0.4824\n","Epoch 4/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.5775 - sparse_categorical_accuracy: 0.5265\n","Epoch 5/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.4782 - sparse_categorical_accuracy: 0.5519\n","Epoch 6/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.4141 - sparse_categorical_accuracy: 0.5687\n","Epoch 7/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.3631 - sparse_categorical_accuracy: 0.5811\n","Epoch 8/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.3198 - sparse_categorical_accuracy: 0.5935\n","Epoch 9/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.2754 - sparse_categorical_accuracy: 0.6044\n","Epoch 10/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.2409 - sparse_categorical_accuracy: 0.6133\n","Epoch 11/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.1966 - sparse_categorical_accuracy: 0.6265\n","Epoch 12/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.1670 - sparse_categorical_accuracy: 0.6341\n","Epoch 13/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.1181 - sparse_categorical_accuracy: 0.6482\n","Epoch 14/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.0795 - sparse_categorical_accuracy: 0.6598\n","Epoch 15/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.0313 - sparse_categorical_accuracy: 0.6738\n","Epoch 16/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.9873 - sparse_categorical_accuracy: 0.6877\n","Epoch 17/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.9341 - sparse_categorical_accuracy: 0.7045\n","Epoch 18/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.9010 - sparse_categorical_accuracy: 0.7141\n","Epoch 19/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8584 - sparse_categorical_accuracy: 0.7275\n","Epoch 20/20\n","\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8309 - sparse_categorical_accuracy: 0.7355\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ac09390f310>"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Now let's see what this model produces as output. To make this work, we need to implement the feedback mechanisms for auto-regressive text production."],"metadata":{"id":"Jr_DGeWlMp5N"}},{"cell_type":"code","source":["# We need two input layers for our generative model.\n","# One input layer will receive a single character token (the last prediction)\n","# The second input layer will receive the hidden / embedded representation of the sequence as of the last prediction that was made.\n","# We are going to udpate and carry the hidden state forward into each subsequent prediction.\n","# This is kind of like what an RNN does internally, but we are doing it manually outside of the model.\n","\n","# Note that our generation model will take only 1 token as input.\n","inputs = keras.Input(shape=(1,), dtype=\"int\", name=\"integer_seq_input\")\n","\n","# It also takes a 1,024 embedded representation of the sequence (up to the point of the current prediction) as additional input.\n","input_state = keras.Input(shape=(1024,), name=\"state\")\n","\n","# The last token is passed through our embedding layer\n","x = layers.Embedding(vocabulary_size, 256)(inputs)\n","\n","# The 1,024 embedded representation gets passed into the GRU, along with the embedding of the last token we produced.\n","# Note that this GRU does NOT return sequences, so it produces a single token output at the end of the text sequence, rather than output for every input token (like we did in our training model).\n","x, output_state = layers.GRU(1024, return_state=True)(x, initial_state=input_state)\n","\n","# We predict the next token.\n","outputs = layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n","\n","# And our model will return i) the next token prediction, as well as ii) the current 1,024 dimensional representation of the sequence at the point of the new prediction.\n","generation_model = keras.Model(\n","    inputs=(inputs, input_state),\n","    outputs=(outputs, output_state),\n",")\n","\n","# This function does a fuzzy match essentially between the old model and the new one, and copies weights where it can\n","# So, it will take the Embedding layer weights from the old model, and look for the first embedding layer in the new model\n","# where the shape is the same, and copy weights over as soon as it sees that.\n","# The same happens with the GRU weights. Layers in the new model with no match in the old model have randomly initialized weights.\n","\n","# So, the GRU layer and embedding layer in our generative model now have meaningful weights baked into them.\n","generation_model.set_weights(model.get_weights())"],"metadata":{"id":"M8lyN7vhMtc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare data and dictionaries to work with our generative model..."],"metadata":{"id":"dOoJEnm5M618"}},{"cell_type":"code","source":["tokens = tokenizer.get_vocabulary()\n","token_ids = range(vocabulary_size)\n","\n","# We are making dictionaries that we will use to go back and forth from integer indices to readable words...\n","char_to_id = dict(zip(tokens, token_ids))\n","id_to_char = dict(zip(token_ids, tokens))\n","\n","# Our starting prompt for the language model...\n","# We will pass these tokens through our generative model to 'burn in' a starting value for the hidden state, so the model can then produce something meaningful as output.\n","prompt = \"\"\"\n","KING RICHARD III:\n","\"\"\""],"metadata":{"id":"M6PD_3s2M_md"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we 'burn in' the generative model, so it can learn a meaningful 'state' value to start with..."],"metadata":{"id":"mgzlb-tDNdFY"}},{"cell_type":"code","source":["input_ids = [char_to_id[c] for c in prompt]\n","\n","# We start with a state of 0, and we will update the state vector by passing over the first several characters.\n","state = keras.ops.zeros(shape=(1, 1024))\n","\n","for token_id in input_ids:\n","    inputs = keras.ops.expand_dims([token_id], axis=0)\n","    predictions, state = generation_model((inputs, state))\n","\n","    # At each token, we can see what it is predicting to begin with, and we see it is making mistakes during the burn-in period.\n","    # But this is the generative model essentially first calibrating to the prompt.\n","    print(id_to_char[np.argmax(predictions)])\n","    state = keras.ops.expand_dims(state, axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8X-DdsQNgQ_","executionInfo":{"status":"ok","timestamp":1744817520002,"user_tz":240,"elapsed":215,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"e34e5c3c-f67d-44fa-f2d9-696a98b086a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["T\n","I\n","N\n","G\n"," \n","H\n","I\n","C\n","H\n","A\n","R\n","D\n"," \n","I\n","I\n",":\n",":\n","\n","\n","S\n"]}]},{"cell_type":"markdown","source":["Now we can use the 'calibrated' model to produce subsequent text."],"metadata":{"id":"j4WruUptOqVx"}},{"cell_type":"code","source":["import numpy as np\n","\n","# We will append our autoregressive predictions into a list, one at a time.\n","generated_ids = []\n","\n","# Let's produce 250 tokens of output.\n","max_length = 250\n","\n","for i in range(max_length):\n","    next_char = int(np.array(keras.ops.argmax(predictions, axis=-1)[0]))\n","    generated_ids.append(next_char)\n","    inputs = keras.ops.expand_dims([next_char], axis=0)\n","\n","    # Reshape the state vector so its the right dimensionality for our input layer.\n","    state = keras.ops.expand_dims(state, axis=0)\n","    predictions, state = generation_model((inputs, state))\n","\n","# We will now join all the predicted characters together and add them to the original prompt, printing the result out.\n","output = \"\".join([id_to_char[token_id] for token_id in generated_ids])\n","print(prompt + output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeDlbq-qOqFW","executionInfo":{"status":"ok","timestamp":1744817759387,"user_tz":240,"elapsed":2710,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"3793c87c-f1f4-43d8-d608-f9e9e3ef10e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","KING RICHARD III:\n","'ll play the morning of this presence\n","Master of a man and husbandry.\n","\n","AUFIDIUS:\n","I know you well.\n","\n","LADY ANNE:\n","What thou art too heart?\n","O, the part is well as false as dangerous too:\n","Thou art too hate him here as leave it is and least\n","As they are but o\n"]}]}]}