{"cells":[{"cell_type":"markdown","metadata":{"id":"AboS-ONzFp22"},"source":["# **Retrieval Augmented Generation with LLMs**"]},{"cell_type":"markdown","source":["#Import Libraries and Load an LLM"],"metadata":{"id":"ClLTYQaf3klu"}},{"cell_type":"markdown","source":["We will install updated versions of Keras hub, the latest pre-release updates for Keras (keras-nightly), and pypdf2, which we can use to extract text from PDF files (our reference database in our RAG)."],"metadata":{"id":"P7zZgFTE0I_J"}},{"cell_type":"code","source":["!pip install --upgrade --quiet keras-hub-nightly keras-nightly\n","!pip install --upgrade --quiet keras-hub keras-nlp\n","!pip install --quiet pypdf2"],"metadata":{"id":"lDyCKEfu36Uv","executionInfo":{"status":"ok","timestamp":1744894462100,"user_tz":240,"elapsed":29366,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c532923-6380-42d9-914c-ee152b08b153"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import keras_hub\n","import keras_nlp\n","import numpy as np\n","import pandas as pd\n","import keras\n","import keras_hub\n","from PyPDF2 import PdfReader\n","from sklearn.metrics.pairwise import cosine_similarity # We will use cosine similarity to implement our document lookup."],"metadata":{"id":"hjJpVaVdJAFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's load a pre-trained LLM from Keras hub. We will use Falcon 1B (because it is not too big)."],"metadata":{"id":"DhVQTeGX0o_3"}},{"cell_type":"code","source":["# Load a Model with float16 precision. This step may takes some time.\n","falcon_chat = keras_hub.models.CausalLM.from_preset(\n","    \"falcon_refinedweb_1b_en\",#\"phi3_mini_4k_instruct_en\",\n","    dtype=\"float16\"\n",")"],"metadata":{"id":"KXrA_BuoI-6m","executionInfo":{"status":"ok","timestamp":1744895100308,"user_tz":240,"elapsed":145333,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5a0c2f8-7d9d-491e-d1fc-bb33bd643a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/falcon/keras/falcon_refinedweb_1b_en/2/download/config.json...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 508/508 [00:00<00:00, 1.31MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/falcon/keras/falcon_refinedweb_1b_en/2/download/model.weights.h5...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.89G/4.89G [01:38<00:00, 53.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/falcon/keras/falcon_refinedweb_1b_en/2/download/tokenizer.json...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 628/628 [00:00<00:00, 1.24MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/falcon/keras/falcon_refinedweb_1b_en/2/download/assets/tokenizer/vocabulary.json...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 0.99M/0.99M [00:00<00:00, 2.76MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/models/keras/falcon/keras/falcon_refinedweb_1b_en/2/download/assets/tokenizer/merges.txt...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 446k/446k [00:00<00:00, 1.58MB/s]\n"]}]},{"cell_type":"markdown","source":["We will ask Falcon to answer a question for us about Moby Dick, namely 'who was the first mate on Capt. Ahab's boat?' The correct answer is Starbuck."],"metadata":{"id":"KW-dIneb1rCr"}},{"cell_type":"code","source":["prompt = \"What was the name of the first mate on Captain Ahab's boat?\"\n","\n","print(falcon_chat.generate(prompt, max_length=64))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhQbZqbuN5I5","executionInfo":{"status":"ok","timestamp":1744901207733,"user_tz":240,"elapsed":20398,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"3a3ac860-c3cb-4e21-cfcd-98f5788d5876"},"execution_count":174,"outputs":[{"output_type":"stream","name":"stdout","text":["What was the name of the first mate on Captain Ahab's boat?\n","- print Print Print\n","- list Cite\n","1 Answer\n","The first mate was a man named Queequinne. He was a native of England and he was the son of a man named John Queinne and a native\n"]}]},{"cell_type":"markdown","source":["#Implement RAG with this Model"],"metadata":{"id":"JnC16cfe3TQe"}},{"cell_type":"markdown","source":["Write a function to extract the text from some PDF files."],"metadata":{"id":"7EVqnFpp3vzK"}},{"cell_type":"code","source":["import re\n","from PyPDF2 import PdfReader\n","\n","# 500-token chunks of mutually exclusive text (no overlap between chunks)\n","def process_pdf(pdf_path, chunk_size=500, overlap=0):\n","    reader = PdfReader(pdf_path)\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text() + \" \"\n","\n","    # Preprocessing\n","    text = re.sub(r'\\r\\n|\\r|\\n', ' ', text)  # Remove carriage returns and newlines\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","\n","    if len(text) <= chunk_size:\n","        return [text]\n","\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = min(start + chunk_size, len(text))\n","        if end < len(text) and text[end] != ' ':\n","            end = text.rfind(' ', start, end) + 1\n","        chunks.append(text[start:end])\n","        if end == len(text):\n","            break\n","        start = end - overlap\n","\n","    return chunks"],"metadata":{"id":"6qmJFMv0e6He","executionInfo":{"status":"ok","timestamp":1744902241441,"user_tz":240,"elapsed":3,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":190,"outputs":[]},{"cell_type":"markdown","source":["Let's find a copy of Moby Dick that we can chunk and use for RAG."],"metadata":{"id":"pekgpGXm4sMp"}},{"cell_type":"code","source":["# Download PDF of a cookbook recipe and save it locally for processing.\n","!wget -O moby_dick.pdf https://uberty.org/wp-content/uploads/2015/12/herman-melville-moby-dick.pdf\n","\n","!apt-get install -y poppler-utils\n","\n","# Extract just the first 100 pages... it's a long book. Starbuck is first mentioned in the table of contents (though not in reference to being the first mate) and then again on page 77.\n","!pdftotext -f 1-50 moby_dick.pdf moby_dick.txt\n","\n","pdf_path = \"moby_dick.pdf\"\n","chunks = process_pdf(pdf_path)\n","\n","print(chunks[-2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01aX3tLO4uOC","executionInfo":{"status":"ok","timestamp":1744902253789,"user_tz":240,"elapsed":9346,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"e1cc622e-78c1-4885-eab1-590a6d79ba24"},"execution_count":191,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-17 15:04:04--  https://uberty.org/wp-content/uploads/2015/12/herman-melville-moby-dick.pdf\n","Resolving uberty.org (uberty.org)... 68.66.200.199\n","Connecting to uberty.org (uberty.org)|68.66.200.199|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1544566 (1.5M) [application/pdf]\n","Saving to: ‘moby_dick.pdf’\n","\n","moby_dick.pdf       100%[===================>]   1.47M  3.80MB/s    in 0.4s    \n","\n","2025-04-17 15:04:05 (3.80 MB/s) - ‘moby_dick.pdf’ saved [1544566/1544566]\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n","0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n","pdftotext version 22.02.0\n","Copyright 2005-2022 The Poppler Developers - http://poppler.freedesktop.org\n","Copyright 1996-2011 Glyph & Cog, LLC\n","Usage: pdftotext [options] <PDF-file> [<text-file>]\n","  -f <int>             : first page to convert\n","  -l <int>             : last page to convert\n","  -r <fp>              : resolution, in DPI (default is 72)\n","  -x <int>             : x-coordinate of the crop area top left corner\n","  -y <int>             : y-coordinate of the crop area top left corner\n","  -W <int>             : width of crop area in pixels (default is 0)\n","  -H <int>             : height of crop area in pixels (default is 0)\n","  -layout              : maintain original physical layout\n","  -fixed <fp>          : assume fixed-pitch (or tabular) text\n","  -raw                 : keep strings in content stream order\n","  -nodiag              : discard diagonal text\n","  -htmlmeta            : generate a simple HTML file, including the meta information\n","  -enc <string>        : output text encoding name\n","  -listenc             : list available encodings\n","  -eol <string>        : output end-of-line convention (unix, dos, or mac)\n","  -nopgbrk             : don't insert page breaks between pages\n","  -bbox                : output bounding box for each word and page size to html.  Sets -htmlmeta\n","  -bbox-layout         : like -bbox but with extra layout bounding box data.  Sets -htmlmeta\n","  -cropbox             : use the crop box rather than media box\n","  -colspacing <fp>     : how much spacing we allow after a word before considering adjacent text to be a new column, as a fraction of the font size (default is 0.7, old releases had a 0.3 default)\n","  -opw <string>        : owner password (for encrypted files)\n","  -upw <string>        : user password (for encrypted files)\n","  -q                   : don't print any messages or errors\n","  -v                   : print copyright and version info\n","  -h                   : print usage information\n","  -help                : print usage information\n","  --help               : print usage information\n","  -?                   : print usage information\n","by reason of its cunning spring and owing to its great buoyancy rising with great force the coﬃn likebuoy shot lengthwise from the sea fell over and ﬂoated by my side Buoyed up by that coﬃn for almost one whole day and night I ﬂoated on a soft and dirgelike main e unharming sharks they glided by as if with padlocks on their mouths the savage seahawks sailed with sheathed beaks On the second day a sail drew near nearer and picked me up at last It was the deviouscruising Rachel that in her \n"]}]},{"cell_type":"code","source":["# Find the first text chunk that mentions 'starbuck'\n","\n","starbuck_indices = []\n","for i,chunk in enumerate(chunks):\n","    if 'starbuck' in chunk.lower():\n","        starbuck_indices.append(i)\n","\n","# Starbuck is mentioned in the table of contents...\n","print(starbuck_indices)\n","print(chunks[starbuck_indices[1]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYfDDd2g5mWh","executionInfo":{"status":"ok","timestamp":1744902277477,"user_tz":240,"elapsed":7,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"8ba07607-3bd7-4d1e-d47f-4398a4cf8533"},"execution_count":192,"outputs":[{"output_type":"stream","name":"stdout","text":["[4, 370, 372, 373, 375, 385, 386, 387, 388, 415, 417, 419, 420, 421, 422, 424, 437, 438, 555, 562, 565, 610, 614, 615, 618, 619, 621, 622, 623, 627, 628, 633, 636, 637, 640, 642, 647, 649, 706, 817, 818, 840, 843, 872, 873, 1049, 1070, 1107, 1146, 1149, 1194, 1195, 1196, 1197, 1294, 1302, 1304, 1309, 1321, 1326, 1327, 1329, 1332, 1431, 1441, 1444, 1445, 1452, 1617, 1619, 1749, 1752, 1753, 1756, 1758, 1759, 1760, 1761, 1779, 1821, 1851, 1852, 1854, 1855, 1856, 1857, 1858, 1859, 1861, 1862, 1866, 1867, 1876, 1877, 1879, 1883, 1885, 1887, 1893, 1894, 1898, 1902, 1903, 1905, 1915, 1916, 1917, 1923, 1942, 1953, 1962, 1963, 1965, 1978, 1979, 1980, 1982, 1983, 1984, 1985, 1989, 1990, 1996, 1997, 2025, 2026, 2027, 2036, 2037, 2038, 2041, 2042, 2045, 2047, 2054, 2055, 2065]\n"," with a sort of muﬄedness then seemed troubled in the nose then revolved over once or twice then sat up and rubbed his eyes Holloa he breathed at last who be ye smokers Shipped men answered I when does she sail Aye aye ye are going in her be ye She sails today e Captain came aboard last night What CaptainAhab Who but him indeed I was going to ask him some further questions concerning Ahab when we heard a noise on deck Holloa Starbucks astir said the rigger Hes a lively chief mate that good man \n"]}]},{"cell_type":"markdown","source":["We need to construct embeddings from our prompt and from our text chunks to be able to perform the lookup between our prompt and the recipe text chunks. Here is how we can recover a Falcon embedding for the prompt..."],"metadata":{"id":"S_uCQk506Dim"}},{"cell_type":"code","source":["import tensorflow as tf\n","from keras_hub.models import CausalLM\n","from keras_hub.tokenizers import FalconTokenizer\n","\n","# Load model and tokenizer\n","tokenizer = FalconTokenizer.from_preset(\"falcon_refinedweb_1b_en\")\n","\n","# Tokenize input (returns just token IDs, shape: (seq_len,))\n","token_ids = tokenizer(prompt)\n","\n","# Add batch dimension (now shape: (1, seq_len))\n","token_ids = tf.expand_dims(token_ids, 0)\n","\n","# Create dummy padding mask of ones (no padding here for a single prompt, but the LLM expects a masking tensor along with the input, so it knows what tokens it can ignore.)\n","padding_mask = tf.ones_like(token_ids)\n","\n","# Pass both token_ids and padding_mask to the backbone\n","hidden = falcon_chat.backbone({\n","    \"token_ids\": token_ids,\n","    \"padding_mask\": padding_mask\n","})\n","\n","# Mean-pool to get a sentence embedding\n","mask = tf.cast(padding_mask, tf.float16)\n","masked_hidden = hidden * tf.expand_dims(mask, axis=-1)\n","embedding = tf.reduce_sum(masked_hidden, axis=1) / tf.reduce_sum(mask, axis=1, keepdims=True)\n","\n","print(\"Prompt embedding shape:\", embedding.shape)  # (1, hidden_dim)\n","print(f\"Prompt embedding: {embedding}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7WSl6tk6gEa","executionInfo":{"status":"ok","timestamp":1744902288288,"user_tz":240,"elapsed":6690,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"5e5ee52d-77bf-49b3-9950-51813cd4184e"},"execution_count":193,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt embedding shape: (1, 2048)\n","Prompt embedding: [[ 0.956   3.248   2.383  ...  2.896  -1.5625 -3.219 ]]\n"]}]},{"cell_type":"markdown","source":["And we will now do the same thing for all chunks fo text that came from our reference copy of Moby Dick. We will first tokenize each chunk... note that this may take some time because our chunks span overlapping sections of 50 pages of text :)."],"metadata":{"id":"wi0zANesAjnQ"}},{"cell_type":"code","source":["tokenized_ids = [tokenizer(chunk) for chunk in chunks]\n","max_len = max(len(ids) for ids in tokenized_ids)\n","\n","print(f'Our longest chunk has {max_len} tokens')"],"metadata":{"id":"f_ngueG38lbq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744904685639,"user_tz":240,"elapsed":2370822,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"f726e734-cd07-4aea-aea0-4b54b4079787"},"execution_count":194,"outputs":[{"output_type":"stream","name":"stdout","text":["Our longest chunk has 174 tokens\n"]}]},{"cell_type":"markdown","source":["Now we will recover embeddings for each chunk, using Falcon's backbone, in batches of 4. The reason we do 4 chunks at a time is that the tensor will otherwise be huge and it will result in GPU memory errors."],"metadata":{"id":"6GQPmmmvBkJ7"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def get_embeddings_in_batches(token_ids, padding_mask, model, batch_size=4):\n","    embeddings_list = []\n","    for i in range(0, token_ids.shape[0], batch_size):\n","        batch_ids = token_ids[i:i+batch_size]\n","        batch_mask = padding_mask[i:i+batch_size]\n","\n","        inputs = {\n","            \"token_ids\": batch_ids,\n","            \"padding_mask\": batch_mask\n","        }\n","\n","        hidden = model.backbone(inputs)  # shape: (batch_size, seq_len, hidden_dim)\n","\n","        mask = tf.cast(batch_mask, tf.float16)\n","        masked_hidden = hidden * tf.expand_dims(mask, axis=-1)\n","        embedding = tf.reduce_sum(masked_hidden, axis=1) / tf.reduce_sum(mask, axis=1, keepdims=True)\n","        embeddings_list.append(embedding)\n","\n","    return tf.concat(embeddings_list, axis=0)\n","\n","\n","# Pad sequences to same length\n","padded_ids = pad_sequences(tokenized_ids, padding='post')  # shape: (num_chunks, max_seq_len)\n","\n","# Create padding mask (1 where there's a token, 0 where there's padding)\n","padding_mask = (padded_ids != 0).astype(\"int32\")\n","\n","# Convert to tensors\n","token_ids = tf.convert_to_tensor(padded_ids, dtype=tf.int32)\n","padding_mask = tf.convert_to_tensor(padding_mask, dtype=tf.int32)\n","\n","# Now run batching\n","embeddings = get_embeddings_in_batches(token_ids, padding_mask, falcon_chat, batch_size=4)"],"metadata":{"id":"2h2KN2sMBJeW","executionInfo":{"status":"ok","timestamp":1744905358128,"user_tz":240,"elapsed":335195,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":195,"outputs":[]},{"cell_type":"markdown","source":["Here are our document chunk embeddings..."],"metadata":{"id":"Lg4oKFcXAokH"}},{"cell_type":"code","source":["chunk_index = 100\n","\n","# Directly access and print the first chunk\n","print(f'CHUNK TEXT: \\n\\n{chunks[chunk_index]} \\n\\n')\n","\n","# Count words directly\n","print(f'CHUNK DETAILS: \\n\\nThe chunk contains roughly {len(chunks[chunk_index].split(\" \"))} words (based on white spaces).')\n","\n","# Look at embedding shape directly\n","print(f'Its embedding has {len(embeddings[chunk_index])} dimensions.\\n')\n","\n","# Print vector representation directly\n","print(f'VECTOR REPRESENTATION:\\n')\n","print(embeddings[chunk_index])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_bJ9TOB2TQj","executionInfo":{"status":"ok","timestamp":1744905369657,"user_tz":240,"elapsed":45,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"35c63902-734f-41f6-bcbe-2b7900be7cda"},"execution_count":197,"outputs":[{"output_type":"stream","name":"stdout","text":["CHUNK TEXT: \n","\n","uncomfortableness and seeing him now exhibiting strong symptoms of concluding his business operations and jump ing into bed with me I thought it was high time now or never before the light was put out to break the spell into which I had so long been bound But the interval I spent in deliberating what to say was a fatal one Taking up his tomahawk from the table he examined the head of it for an instant and then holding it to the light with his mouth at the handle he puﬀed out great clouds of  \n","\n","\n","CHUNK DETAILS: \n","\n","The chunk contains roughly 97 words (based on white spaces).\n","Its embedding has 2048 dimensions.\n","\n","VECTOR REPRESENTATION:\n","\n","tf.Tensor([ 1.287  2.217  2.96  ...  2.605 -2.014 -2.877], shape=(2048,), dtype=float16)\n"]}]},{"cell_type":"markdown","source":["Let's combine text chunks (column 1) with associated embeddings (column 2) in a pandas dataframe and save it.\n"],"metadata":{"id":"KGwVehQSC1fY"}},{"cell_type":"code","source":["rag_df = pd.DataFrame({'chunk': chunks, 'embedding': embeddings.numpy().tolist()})"],"metadata":{"id":"EeaJHCCpdUVx","executionInfo":{"status":"ok","timestamp":1744905374358,"user_tz":240,"elapsed":318,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":198,"outputs":[]},{"cell_type":"markdown","source":["We can save it to our drive folder for later use (or we can load a previously stored file)..."],"metadata":{"id":"uF3lP7JZdVDV"}},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","rag_df.to_pickle('/content/drive/MyDrive/Teaching/Courses/BA 865/BA865-2025/Lecture Materials/Week 5/rag_df.pkl')\n","#rag_df = pd.read_pickle('/content/drive/MyDrive/Teaching/Courses/BA 865/BA865-2025/Lecture Materials/Week 5/rag_df.pkl')"],"metadata":{"id":"GU5WlH1EC6k8","executionInfo":{"status":"ok","timestamp":1744905440612,"user_tz":240,"elapsed":2379,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f557f37-fc21-49d2-8219-dfc20ecb501d"},"execution_count":202,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Now we need a method to perform a top-k similar chunk lookup (relative to the prompt)."],"metadata":{"id":"TDdEE_z5Cbu_"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def rag_lookup(query, rag_data, top_k=2):\n","\n","    token_ids = tokenizer(query)\n","    token_ids = tf.expand_dims(token_ids, 0)\n","    padding_mask = tf.ones_like(token_ids)\n","\n","    hidden = falcon_chat.backbone({\n","        \"token_ids\": token_ids,\n","        \"padding_mask\": padding_mask\n","    })\n","    mask = tf.cast(padding_mask, tf.float16)\n","    masked_hidden = hidden * tf.expand_dims(mask, axis=-1)\n","    query_embedding = tf.reduce_sum(masked_hidden, axis=1) / tf.reduce_sum(mask, axis=1, keepdims=True)\n","\n","    # Stack chunk embeddings into a matrix\n","    chunk_embeddings = np.vstack(rag_data['embedding'].values)\n","\n","    # Compute cosine similarity between query and all chunks\n","    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n","\n","    # Get top results\n","    temp_df = rag_data.copy()\n","    temp_df['similarity'] = similarities\n","    results = temp_df.sort_values('similarity', ascending=False).head(top_k)\n","    return results\n","\n","lookup = rag_lookup(\"Who was the chief mate on ahab's ship?\", rag_df, top_k = 10)\n","lookup['chunk'].iloc[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"I-eFClmICs7S","executionInfo":{"status":"ok","timestamp":1744905818713,"user_tz":240,"elapsed":1270,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"0c32544c-fe09-4b4c-bc01-ceecb086d145"},"execution_count":215,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'and Star bucks coerced will were Ahabs so long as Ahab kept his magnet at Starbucks brain still he knew that for all this the chief mate in his soul abhorred his cap tains quest and could he would joyfully disintegrate himself from it or even frustrate it it might be that a long interval would elapse ere the White Whale was seen During that long interval Starbuck would ever be apt to fall into open relapses of rebellion against his captains leadership unless some ordinary pruden tial '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":215}]},{"cell_type":"code","source":["# RAG prompt creation\n","def create_rag_prompt(query, rag_data, top_k=2):\n","    results = rag_lookup(query, rag_data, top_k)\n","    prompt = f\"Answer the following question: {query}\\n\\n\"\n","    prompt += \"Here are some recent relevant documents:\\n\"\n","    for i, (_, row) in enumerate(results.iterrows()):\n","        prompt += f\"\\n--- Source {i+1} ---\\n{row['chunk']}\\n\"\n","    prompt += \"\\nPlease refer to the relevant documents provided above when answering.\"\n","    return prompt"],"metadata":{"id":"cwSJYado3-vW","executionInfo":{"status":"ok","timestamp":1744905573032,"user_tz":240,"elapsed":5,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":207,"outputs":[]},{"cell_type":"code","source":["rag_prompt = create_rag_prompt(\"Who was the chief mate on ahab's ship?\", rag_df, top_k=3)\n","print(rag_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUS_PZCj7bcr","executionInfo":{"status":"ok","timestamp":1744905829850,"user_tz":240,"elapsed":1479,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"cbf568c8-7bab-4d51-93a7-d1b168ad82ee"},"execution_count":217,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer the following question: Who was the chief mate on ahab's ship?\n","\n","Here are some recent relevant documents:\n","\n","--- Source 1 ---\n","and Star bucks coerced will were Ahabs so long as Ahab kept his magnet at Starbucks brain still he knew that for all this the chief mate in his soul abhorred his cap tains quest and could he would joyfully disintegrate himself from it or even frustrate it it might be that a long interval would elapse ere the White Whale was seen During that long interval Starbuck would ever be apt to fall into open relapses of rebellion against his captains leadership unless some ordinary pruden tial \n","\n","--- Source 2 ---\n","tormented chase of that demon phantom that some time or other swims before all human hearts while chasing such over this round globe they either lead us on in barren mazes or midway leave us whelmed e cabincompass is called the telltale because without going to the compass at the helm the Captain while below can inform himself of the course of the ship  CHAPTER L THE GAM e ostensible reason why Ahab did not go on board of the whaler we had spoken was this the wind and sea betokened storms But \n","\n","--- Source 3 ---\n","gregated caravan Moby Dick himself might not temporarily be swimming like the worshipped whiteelephant in the coronation procession of the Siamese So with stunsail piled on stunsail we sailed along driving these Leviathans before us when of a sudden the voice of Tashtego was heard loudly direing attention to something in our wake Corresponding to the crescent in our van we beheld another in our rear It seemed formed of detached white vapors rising and falling something like the spouts of the \n","\n","Please refer to the relevant documents provided above when answering.\n"]}]},{"cell_type":"markdown","source":["And now we can see how the process would work, to provide additional context to the LLM in the prompt when obtaining its answer. Though, Falcon... struggles. The context window is a bit of a challenge here."],"metadata":{"id":"1B0RG6kzgxCQ"}},{"cell_type":"code","source":["# Without RAG\n","prompt = \"Name the chief mate on Ahabs ship.\"\n","\n","response_without_rag = None\n","response_without_rag = falcon_chat.generate(prompt, max_length=200)\n","\n","# With RAG\n","rag_prompt = create_rag_prompt(prompt, rag_df, top_k=1)\n","\n","response_with_rag = None\n","response_with_rag = falcon_chat.generate(rag_prompt, max_length=200)\n","\n","# Print responses\n","print(\"WITHOUT RAG:\")\n","print(response_without_rag)\n","print(\"\\n\\nWITH RAG:\")\n","print(response_with_rag)"],"metadata":{"id":"mqm4sAzM9HBL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744907567155,"user_tz":240,"elapsed":4538,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"86182e02-5c8e-49e1-8169-fdac9cbed1d5"},"execution_count":230,"outputs":[{"output_type":"stream","name":"stdout","text":["WITHOUT RAG:\n","Name the chief mate on Ahabs ship.\n","1) Captain Ahab\n","2) Captain Queeg\n","3) Captain Smith\n","4) First Officer Queeg\n","5) First Officer Queeg\n","6) Captain Smith\n","7) First Officer Smith\n","8) First Officer Smith\n","9) Captain Smith\n","10) First Officer\n","11) Captain Queeg\n","12) Captain Queeg\n","13) First Officer\n","14) First Officer\n","15) Captain Smith\n","16) Captain Queeg\n","17) First Officer\n","18) Captain Ahab\n","19) Captain Ahab\n","20) Captain Queeg\n","21) Captain Ahab\n","22) First Officer\n","23) Captain Ahab\n","24) Queeg\n","25) Queeg\n","26) Captain Ahab\n","27) Captain Ahab\n","\n","\n","WITH RAG:\n","Answer the following question: Name the chief mate on Ahabs ship.\n","\n","Here are some recent relevant documents:\n","\n","--- Source 1 ---\n","and Star bucks coerced will were Ahabs so long as Ahab kept his magnet at Starbucks brain still he knew that for all this the chief mate in his soul abhorred his cap tains quest and could he would joyfully disintegrate himself from it or even frustrate it it might be that a long interval would elapse ere the White Whale was seen During that long interval Starbuck would ever be apt to fall into open relapses of rebellion against his captains leadership unless some ordinary pruden tial \n","\n","Please refer to the relevant documents provided above when answering.\n","Ahae of Ahabs the chief mate was a good mate of Ahab was the mate of Ahabs ship.\n","The answer is Ahabs captain was a good mate Ahab was the ship. Ahab was a mate Ahabs ship.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[{"file_id":"1rFBfnP3hZQSumNvtlGaKNmWmRrqNyCJT","timestamp":1744836539407}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}