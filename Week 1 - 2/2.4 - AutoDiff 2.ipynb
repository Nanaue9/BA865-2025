{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fyFhRCPnDVJk39OOhbBBy1MjfC30mIgF","timestamp":1679414430814}],"authorship_tag":"ABX9TyPhtP9/38/yU0iPqpmr9Lgd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Automatic Differentiation**"],"metadata":{"id":"oV_kcG6Y1T8L"}},{"cell_type":"markdown","source":["In this notebook, I'm going to demonstrate GradientTape(), a function that implements a computation graph, to automate the evaluation of partial derivatives. Here, we define a function, y = x^2, we define a gradientTape, and then we call the gradient() function to evaluate the first derivative of that function with respect to one parameter of that function, in this case x. Note that a tf.Variable() is a tensor that can hold mutable (changeable) values. Here I have defined x as a scalar. "],"metadata":{"id":"3WUDvR031aD_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uo86Hd5d0756","executionInfo":{"status":"ok","timestamp":1679617922103,"user_tz":240,"elapsed":219,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"052966fd-763e-4e16-8444-011f1da74dee"},"outputs":[{"output_type":"stream","name":"stdout","text":["The first derivative of x^2 is 6.0\n"]}],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape:\n","  y = x**2\n","\n","# f(x) = x^2; f'(x) = 2x; numerical evaluation of 2x when x = 3 is 2*3 = 6.\n","print(f'The first derivative of x^2 is {tape.gradient(y,x).numpy()}')"]},{"cell_type":"markdown","source":["We can also nest tapes to take a second derivative, like so:"],"metadata":{"id":"u0E6-nJ9JVkE"}},{"cell_type":"code","source":["with tf.GradientTape() as tape2:\n","  # If we don't specify 'persistent = True' then the tape is dropped from memory after the first gradient is evaluated.\n","  with tf.GradientTape(persistent=True) as tape:\n","     y=x**2\n","\n","  dy_dx = tape.gradient(y,x)\n","  print(f'The first derivative of x^2 is 2x; 2 * 3 = {dy_dx.numpy()}')\n","\n","d2y_dx = tape2.gradient(dy_dx,x)\n","\n","print(f'The second derivative of x^2 is {d2y_dx.numpy()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhBUhCK3JY6f","executionInfo":{"status":"ok","timestamp":1679607667819,"user_tz":240,"elapsed":9,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"864f62fc-19a5-4ced-b85f-0b339be87396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The first derivative of x^2 is 2x; 2 * 3 = 6.0\n","The second derivative of x^2 is 2.0\n"]}]},{"cell_type":"markdown","source":["We can also use tapes with higher dimensional tensors, e.g., here is an example for a sigmoid neuron. In the code below, note that w@x is tensor multiplication. Be careful with the order of the matrices and their dimensions; the matrix multiplication will only work if the shapes align. Also note that reduce_mean() returns the average over elements of y. We can directly obtain the gradient of the loss function w.r.t. our w and b parameters. The resulting gradients will be the same shape as the argument we are taking gradient with respect to, i.e., w.shape and b.shape. So, when we update values in a back pass, we can just calculate w -= dl_dw * learning_rate. and b -= dl_db * learning_rate. "],"metadata":{"id":"O0Rx9a5M3iK5"}},{"cell_type":"code","source":["w = tf.Variable(tf.random.normal((3,2),dtype=tf.float32),name='w')\n","b = tf.Variable(tf.ones(2,dtype=tf.float32),name='b')\n","x = tf.constant([[1., 2., 5.]])\n","\n","# x is a 1x3 vector; w is a 3x2 matrix. So, 1x3 * 3*2 = 1x2 result, and adding a 1x2 vector gives coherent output of the same shape. \n","print(f\"The shape of x*w+b will be: {(x@w+b).numpy().shape}\\n\")\n","\n","with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  z = tf.sigmoid(y)\n","  loss = tf.reduce_mean(z)\n","\n","[dl_dw,dl_db] = tape.gradient(loss,[w,b])\n","print(f'The gradient of loss w.r.t. w is\\n{dl_dw}\\n The gradient w.r.t. b is {dl_db}.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2gu5SDC3mCn","executionInfo":{"status":"ok","timestamp":1679618457406,"user_tz":240,"elapsed":327,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"2ca4bfc8-98c2-4b55-8223-f527bb2a5554"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of x*w+b will be: (1, 2)\n","\n","The gradient of loss w.r.t. w is\n","[[0.00636615 0.00037951]\n"," [0.01273231 0.00075902]\n"," [0.03183077 0.00189754]]\n"," The gradient w.r.t. b is [0.00636615 0.00037951].\n"]}]},{"cell_type":"markdown","source":["Let's do the same thing, but using a dictionary instead of a list..."],"metadata":{"id":"yUHzDpje-p-J"}},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y)\n","\n","my_parms = { \n","    'w': w,\n","    'b': b\n","}\n","\n","gradients = tape.gradient(loss,my_parms)\n","gradients['w']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awtrxduf-uhf","executionInfo":{"status":"ok","timestamp":1679618546641,"user_tz":240,"elapsed":200,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"aa150ada-524a-4ef2-af97-df00e94a3ed2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[0.5, 0.5],\n","       [1. , 1. ],\n","       [2.5, 2.5]], dtype=float32)>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#**Gradient Descent with a Simple Neural Network**"],"metadata":{"id":"WBOW960YYoDp"}},{"cell_type":"markdown","source":["Okay, now let's apply GradientTape to a mock neural network. We have our input layer of x's (4 features), and then a 3-node hidden layer employing ReLU activations, followed by a 4-node hidden layer employing tanh activations, followed by a single output node, employing sigmoid, for a binary DV. Our loss function will be the average of squared predictions. Note that this isn't really a meaningful NN; it's just for show. This definition means we have 3*2 = 6 weights and 2 bias terms. "],"metadata":{"id":"U-mXbcgs_0Zy"}},{"cell_type":"code","source":["# Define our learning rate and our ground truth value for our single training example.\n","learning_rate = 1e-3\n","target=15\n","\n","# Dense implements the operation: output = activation(dot(input, weights) + bias).\n","# Units = 2 means we have two nodes in the layer.\n","# We have 4 inputs, a hidden layer with 2 nodes, thus 4x2 = 8 weights, randomly initialized;\n","# The vector of 2 bias terms will initially default to values of 0.\n","# We then have a sigmoid output layer, e.g., a binary classification scenario, which takes 2 inputs, thus 2*1 = 2 weights, 1 bias term.\n","model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=2, activation=\"relu\", name=\"hiddenLayer\"),\n","        tf.keras.layers.Dense(units=1, activation=\"linear\",name=\"outputLayer\"),\n","])\n","\n","# Here is a random vector of 4 values, e.g., representing a single training observation.\n","input = tf.random.normal((1,4))"],"metadata":{"id":"8rr-fFhN_4dH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can apply GradientTape, in a loop, conducting a forward pass and then recovering gradients for all model parameters, and finally updating our parameters in the opposite direction from the gradients. We will iterate and repeat this process 100 times. Note, if we were doing this with N training examples, we would instead take the average loss across them. So, loss would equal tf.reduce_mean((prediction-target)**2)). We are sticking with a single training data point here for simplicity's sake."],"metadata":{"id":"gyUju8YLMjH0"}},{"cell_type":"code","source":["import numpy as np\n","\n","history = []\n","for i in range(1200):\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    prediction = model(input)\n","    # We define our loss as the square of the forward pass prediction - 3. \n","    # Three is the true value, and we are taking the squared loss with respect to that value.\n","    loss = (prediction-target)**2\n","\n","  # Gradients with respect to every trainable variable, i.e., backward pass.\n","  grad = tape.gradient(loss, model.trainable_variables)\n","  for i in range(len(model.trainable_variables)):\n","    new_parms = model.trainable_variables[i] - grad[i]*learning_rate\n","    model.trainable_variables[i].assign(new_parms)\n","  \n","  history.append(prediction.numpy())\n","\n","# Finally, collapse the list of arrays into a single array.\n","history = np.concatenate(history, axis=0)"],"metadata":{"id":"UrbO4KN-MjkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can plot the optimization process, showing how our model gradually improves until it predicts the correct value."],"metadata":{"id":"ZNGYTEtzThLp"}},{"cell_type":"code","source":["# Plot the evolution of our predictions as we tweak our network's weights and bias.\n","# Eventually, it converges to the correct prediction.\n","plt.plot(range(1200),history,[target]*1200)\n","plt.legend(('Predicted','True'))\n","plt.xlabel('Iteration')\n","plt.ylabel('y value')\n","plt.title('Optimization of a Simple NN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"UEERAnfWTlX-","executionInfo":{"status":"ok","timestamp":1679618916940,"user_tz":240,"elapsed":507,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"2090ad89-4b6d-451c-edbe-338f10ff35c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Optimization of a Simple NN')"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["#**A Simple Example**"],"metadata":{"id":"1fnbHC20LxJ2"}},{"cell_type":"markdown","source":["Use tf.GradientTape() to plot the Hyperbolic Tangent activation function over input values ranging from -10 to 10, and overlaying a plot of the first derivative of the activation function. \n","\n","Notes:\n","\n","* *Any variable you want to calculate a derivative with respect to should be declared as a tf.Variable().*\n","* *To convert a tf tensor into a numpy array, apply the .numpy() function to it.* \n","* *The tanh function is pre-defined for you in tf.nn.tanh(), as are some other activation functions.*"],"metadata":{"id":"69fQkLUSL4Ha"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# Simulating some data for you to initialize x with. This is just a 200x1 array of random values between -10 and +10, sorted.\n","data = tf.sort(tf.random.uniform((200,1),minval=-10,maxval=10),axis=0)\n","\n","x = tf.Variable(data)    ## Define your variable here.\n","\n","# Declare your tape\n","with tf.GradientTape() as tape:\n","  ### Write your y function here.\n","  y = tf.nn.tanh(x)\n","\n","dy_dx = tape.gradient(y,x)\n","\n","# Create your plot here\n","plt.plot(x,y)\n","plt.plot(x,dy_dx)\n","plt.show()"],"metadata":{"id":"fBo6mNhjMDD1","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1680033497364,"user_tz":240,"elapsed":968,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"372ebaef-652b-4194-8fea-76a950777394"},"execution_count":3,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d0f592b0b3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdy_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdy_dx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create your plot here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1053\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"]}]}]}